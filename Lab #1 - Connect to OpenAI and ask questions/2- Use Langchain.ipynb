{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "OPENAI_DAVINCI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DAVINCI_DEPLOYMENT_NAME\")\n",
    "OPENAI_DAVINCI_MODEL_NAME = os.getenv(\"OPENAI_DAVINCI_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "             openai_api_version=OPENAI_DEPLOYMENT_VERSION,\n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             top_p=1,\n",
    "             ):\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      openai_api_version=openai_api_version,\n",
    "                      temperature=temperature,\n",
    "                      max_tokens=max_tokens,\n",
    "                      top_p=top_p,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "                      )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Use Langchain Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " To assess the risk tolerance of a new client, you can use a risk tolerance questionnaire. This questionnaire will ask the client a series of questions about their investment goals, investment experience, and risk preferences. Based on the answers provided, you can determine the client's risk tolerance and recommend an investment strategy that aligns with their risk tolerance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "# create template for prompt using embedded variables\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",question=\"How do you assess the risk tolerance of a new client?\"))\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " It's not clear or the question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asking unrelated question\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",question=\"What's the fastest car in the world? Answer in one sentence. \"))\n",
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
