{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_GPT4o_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4o_DEPLOYMENT_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_GPT4o_DEPLOYMENT_NAME,\n",
    "             deployment_name=OPENAI_GPT4o_DEPLOYMENT_NAME,\n",
    "             openai_api_version=\"2024-02-15-preview\",\n",
    "             temperature=0,\n",
    "             max_tokens=400\n",
    "             ):\n",
    "\n",
    "    llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                            model=model,\n",
    "                            openai_api_version=openai_api_version,\n",
    "                            azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                            temperature=temperature,\n",
    "                            max_tokens=max_tokens\n",
    "                            )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Use Langchain Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Evaluating the performance of Large Language Models (LLMs) in supply chain management involves several key steps:\n",
       "\n",
       "1. **Accuracy and Relevance**: Assess how accurately and relevantly the LLM provides information and solutions to supply chain queries and problems.\n",
       "\n",
       "2. **Efficiency**: Measure the time it takes for the LLM to process and respond to supply chain-related tasks and inquiries.\n",
       "\n",
       "3. **User Satisfaction**: Gather feedback from users interacting with the LLM to determine their satisfaction with the responses and assistance provided.\n",
       "\n",
       "4. **Error Rate**: Track the frequency and types of errors made by the LLM in supply chain contexts to identify areas for improvement.\n",
       "\n",
       "5. **Adaptability**: Evaluate how well the LLM adapts to new supply chain data and evolving scenarios.\n",
       "\n",
       "6. **Integration**: Assess how seamlessly the LLM integrates with existing supply chain management systems and processes.\n",
       "\n",
       "By focusing on these criteria, you can effectively evaluate the performance of LLMs in supply chain management scenarios."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "# create template for prompt using embedded variables\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "message = HumanMessage(\n",
    "    content=prompt_template.format(profession=\"LLms deployer\",  expertise=\"Risk Management\",question=\"How do you evaluate the performance of LLms in a supply chain management scenarios?\")\n",
    ")\n",
    "answer = llm.invoke([message])\n",
    "display(Markdown(answer.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To assess the risk tolerance of a new client, you typically follow these steps:\n",
       "\n",
       "1. **Questionnaire**: Have the client fill out a detailed risk tolerance questionnaire. This will include questions about their financial goals, investment experience, time horizon, and comfort with market fluctuations.\n",
       "\n",
       "2. **Financial Situation**: Review their current financial situation, including income, expenses, assets, liabilities, and any other financial commitments.\n",
       "\n",
       "3. **Investment Goals**: Discuss their short-term and long-term investment goals. Understanding what they aim to achieve helps in determining the appropriate level of risk.\n",
       "\n",
       "4. **Time Horizon**: Determine the time frame for their investments. Generally, a longer time horizon allows for higher risk tolerance.\n",
       "\n",
       "5. **Behavioral Assessment**: Evaluate their past investment behavior, especially how they reacted during market downturns. This can provide insights into their true risk tolerance.\n",
       "\n",
       "6. **Scenario Analysis**: Present different market scenarios and gauge their reactions to potential gains and losses.\n",
       "\n",
       "7. **Regular Reviews**: Risk tolerance can change over time, so itâ€™s important to review and reassess periodically.\n",
       "\n",
       "By combining these methods, you can get a comprehensive understanding of a client's risk tolerance and tailor your investment strategy accordingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "# create template for prompt using embedded variables\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "message = HumanMessage(\n",
    "    content=prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",question=\"How do you assess the risk tolerance of a new client?\")\n",
    ")\n",
    "answer = llm.invoke([message])\n",
    "display(Markdown(answer.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asking unrelated question\n",
    "message = HumanMessage(\n",
    "    content=prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",question=\"What's the fastest car in the world? Answer in one sentence. \")\n",
    ")\n",
    "answer = llm.invoke([message])\n",
    "display(Markdown(answer.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
