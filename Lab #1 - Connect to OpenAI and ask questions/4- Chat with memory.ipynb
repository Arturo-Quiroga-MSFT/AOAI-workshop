{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_GPT4o_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4o_DEPLOYMENT_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_GPT4o_DEPLOYMENT_NAME,\n",
    "             deployment_name=OPENAI_GPT4o_DEPLOYMENT_NAME,\n",
    "             openai_api_version=\"2024-02-15-preview\",\n",
    "             temperature=0,\n",
    "             max_tokens=400\n",
    "             ):\n",
    "\n",
    "    llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                            model=model,\n",
    "                            openai_api_version=openai_api_version,\n",
    "                            azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                            temperature=temperature,\n",
    "                            max_tokens=max_tokens\n",
    "                            )\n",
    "    return llm\n",
    "\n",
    "llm = init_llm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retain conversation history** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Large Language Models (LLMs) are stateless. This means that they don’t retain any information about the conversation history.\n",
    "Each transaction is independent of the previous one. Chatbots keep in memory the conversation history and use it to generate the next response. This is why they are able to generate more coherent responses.\n",
    "\n",
    "Previously we saw that a model fails to answer the question that requires context. We can solve this problem by retaining the conversation history. We can do this by using the LangChain ConversationBufferMemory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "\n",
    "            Current conversation:\n",
    "            {history}\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can override it and set it to \"AI Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(memory_key=\"history\", ai_prefix=\"AI Assistant\", human_prefix=\"Human\"),\n",
    ")\n",
    "res = conversation.invoke(input=\"Hello!\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here's another one for you:\n",
       "\n",
       "Why did the scarecrow win an award?\n",
       "\n",
       "Because he was outstanding in his field!\n",
       "\n",
       "I hope that one gave you a chuckle! If you need more jokes or anything else, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = conversation.invoke(input=\"I need you to tell me a joke!\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here's a Portugal-themed joke for you:\n",
       "\n",
       "Why did the Portuguese football team go to the bank?\n",
       "\n",
       "To get their quarters back!\n",
       "\n",
       "I hope that one made you smile! If you have any more requests or need information about Portugal or anything else, just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = conversation.invoke(input=\"Now do one about Portugal!\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Absolutely, here's a Lisbon-themed joke for you:\n",
       "\n",
       "Why did the tourist bring a ladder to Lisbon?\n",
       "\n",
       "Because they heard the views from the top of the hills are breathtaking!\n",
       "\n",
       "I hope that one gave you a laugh! Lisbon is such a beautiful city with its stunning viewpoints, historic trams, and delicious pastéis de nata. If you have any more requests or need information about Lisbon or anything else, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = conversation.invoke(input=\"I want it to be about Lisbon.\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello!'),\n",
       " AIMessage(content=\"Hi there! How's it going? I'm here to chat and help with whatever you need. What's on your mind today?\"),\n",
       " HumanMessage(content='I need you to tell me a joke!'),\n",
       " AIMessage(content=\"Sure, I'd be happy to! Here's one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\nI hope that brought a smile to your face! If you need another joke or anything else, just let me know!\"),\n",
       " HumanMessage(content='I need you to tell me a joke!'),\n",
       " AIMessage(content=\"Of course! Here's another one for you:\\n\\nWhy did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!\\n\\nI hope that one gave you a chuckle! If you need more jokes or anything else, feel free to ask!\"),\n",
       " HumanMessage(content='Now do one about Portugal!'),\n",
       " AIMessage(content=\"Sure, here's a Portugal-themed joke for you:\\n\\nWhy did the Portuguese football team go to the bank?\\n\\nTo get their quarters back!\\n\\nI hope that one made you smile! If you have any more requests or need information about Portugal or anything else, just let me know!\"),\n",
       " HumanMessage(content='I want it to be about Lisbon.'),\n",
       " AIMessage(content=\"Absolutely, here's a Lisbon-themed joke for you:\\n\\nWhy did the tourist bring a ladder to Lisbon?\\n\\nBecause they heard the views from the top of the hills are breathtaking!\\n\\nI hope that one gave you a laugh! Lisbon is such a beautiful city with its stunning viewpoints, historic trams, and delicious pastéis de nata. If you have any more requests or need information about Lisbon or anything else, feel free to ask!\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.chat_memory.messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More conversation memory types.\n",
    "\n",
    "Keeping full conversation history could be expensive and we can hit the Azure API limits. LangChain provides different types of conversation memory that can be used to keep the conversation history and mitigate the limits issues.\n",
    "\n",
    "**1. ConversationBufferWindowMemory** - keeps the last N messages in the conversation history.\n",
    "\n",
    "**2. ConversationTokenBufferMemory** - keeps the last N tokens in the conversation history.\n",
    "\n",
    "**3. ConversationSummaryBufferMemory** - keeps summary of the conversation over time.\n",
    "\n",
    "Additional Memory Type includes:\n",
    "\n",
    "**4. Vector data memory** - stires the text in a vector DB and retrieves most semantically similar text.\n",
    "\n",
    "**5. Entity memories** - use LLM which rememebers details about specific entities. \n",
    "\n",
    "Note: You can use multiple memories at the same time. \n",
    "\n",
    "Finally you can store the conevrsation history in a conventional database like SQL or NoSQL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: hello\\nHuman: What's your name\\nAI: My name is John\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"What's your name\"},\n",
    "                    {\"output\": \"My name is John\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
