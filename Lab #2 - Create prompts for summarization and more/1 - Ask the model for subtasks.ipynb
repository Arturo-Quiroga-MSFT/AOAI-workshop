{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "             deployment_name=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "             openai_api_version=\"2024-02-15-preview\",\n",
    "             temperature=0,\n",
    "             max_tokens=400\n",
    "             ):\n",
    "\n",
    "    llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                            model=model,\n",
    "                            openai_api_version=openai_api_version,\n",
    "                            azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                            temperature=temperature,\n",
    "                            max_tokens=max_tokens\n",
    "                            )\n",
    "    return llm\n",
    "\n",
    "llm = init_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's do some math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# openAI gets this wrong!!\n",
    "prompt = f\"\"\" Determine if the student's solution delimited by the triple backticks is correct or not.\n",
    "Question: When I was 2 years old my sister was twice my age. I'm now 40 years old how old is my sister now? \n",
    "Student's answer: ```The sister is now 80 years old.``` \n",
    "If student is correct, then write 'Correct', otherwise write 'Incorrect'.\n",
    "\"\"\"\n",
    "\n",
    "sum = llm.invoke(prompt)\n",
    "display(HTML(sum.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask the model to solve it and then to compare both solutions and conclude which is correct.\n",
    " Define the task as a list of sub-tasks and ask the model to perform them in a specific order (splitting the task into sub-tasks technique).   \n",
    " Then ask the model to compare its own solution with the solution provided by the model and conclude which one is correct.\n",
    " Ask the model to share its reasoning for the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Actual solution steps: \n",
       "1 - When I was 2 years old, my sister was twice my age, so she was 4 years old.\n",
       "2 - The age difference between me and my sister is always 2 years.\n",
       "3 - I am now 40 years old, so my sister is 40 - 2 = 38 years old.\n",
       "\n",
       "Student's solution: The sister is now 80 years old.\n",
       "\n",
       "Student's solution is correct: False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Question = f\"\"\"When I was 2 years old my sister was twice of my age . I'm now 40 years old, how old is my sister now?\"\"\"\n",
    "Student_Solution = f\"\"\" The sister is now 80 years old.\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Determine if the Student's Solution for the Question is correct or not.\n",
    "To solve the problem do the following:\n",
    "1 - First, work out your OWN solution to the problem. Evaluate your final result to make sure it is correct and adheres to the question's conditions. \n",
    "    Reason about every step of your solution and make sure it is correct. \n",
    "2 - Second, compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Student's Solution: ```{Student_Solution}```\n",
    "Question: ```{Question}```\n",
    "\n",
    "\n",
    "Use the following output format:\n",
    "Actual solution steps: <your own solution steps>\n",
    "Student's solution: <student's solution>\n",
    "Student's solution is correct: <true/false>\n",
    "\n",
    "\"\"\"\n",
    "sum = llm.invoke(prompt)\n",
    "display(Markdown(sum.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
