{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dschlesinger\\code\\ongoing\\workshops\\AOAI-workshop\\.venv\\lib\\site-packages\\langchain\\embeddings\\azure_openai.py:101: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://aidemos-workshop.openai.azure.com/ to https://aidemos-workshop.openai.azure.com//openai.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dschlesinger\\code\\ongoing\\workshops\\AOAI-workshop\\.venv\\lib\\site-packages\\langchain\\embeddings\\azure_openai.py:108: UserWarning: As of openai>=1.0.0, if `deployment` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dschlesinger\\code\\ongoing\\workshops\\AOAI-workshop\\.venv\\lib\\site-packages\\langchain\\embeddings\\azure_openai.py:116: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://aidemos-workshop.openai.azure.com/ to https://aidemos-workshop.openai.azure.com//openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_MODEL_NAME,\n",
    "    openai_api_base=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    chunk_size = 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run ONLY ONCE to create the embeddings - Split text into chunks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages:  292\n"
     ]
    }
   ],
   "source": [
    "fileName = \"./data/fabric-data-engineering.pdf\"\n",
    "loader = PyPDFLoader(fileName)\n",
    "pages = loader.load_and_split()\n",
    "print(\"Number of pages: \", len(pages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run ONLY ONCE to create the embeddings - Create embeddings and save to FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=pages, embedding=embeddings)\n",
    "# save the FAISS index to disk\n",
    "db.save_local(\"./dbs/documentation/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "def call_openAI(text):\n",
    "    # prepare prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer in a clear and concise manner.\"},\n",
    "                {\"role\": \"user\", \"content\": text}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_DEPLOYMENT_NAME,\n",
    "        messages = messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initialize retrieval API WITH your data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vector store to memory\n",
    "vectorStore = FAISS.load_local(\"./dbs/documentation/faiss_index\", embeddings)\n",
    "retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})  # returns 2 most similar vectors/documents\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ask questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can I use PowerBI datamart for 110 GB data volume? Explain which other options are available for data volumes of 110 GB?',\n",
       " 'result': ' No, PowerBI datamart is only suitable for data volumes up to 100 GB. For data volumes of 110 GB, you can use a data warehouse or a lakehouse in Microsoft Fabric. \\n\\nData warehouse is suitable for structured data, and is ideal for SQL engineers and data warehouse developers. It is organized by databases, schemas, and tables, and read operations can be performed using Spark and T-SQL. Write operations can be performed using T-SQL.\\n\\nLakehouse is suitable for unstructured, semi-structured, and structured data, and is ideal for data engineers and data scientists. It is organized by folders and files, databases, and tables, and read operations can be performed using Spark, T-SQL, and Power BI. Write operations can be performed using Spark (Scala, PySpark, Spark SQL, R) and T-SQL. \\n\\nBoth data warehouse and lakehouse can handle unlimited data volume. \\n\\nNote that Microsoft Fabric is currently in PREVIEW, and the information provided may be subject to change.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"query\": \"Can I use PowerBI datamart for 110 GB data volume? Explain which other options are available for data volumes of 110 GB?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the steps to load a CSV file to a delta table in Microsoft Fabric?',\n",
       " 'result': ' The steps to load a CSV file to a delta table in Microsoft Fabric are:\\n1. Select Synapse Data Engineering experience in Microsoft Fabric.\\n2. Make sure you are in the desired workspace or select/create one.\\n3. Select the Lakehouse icon under the New section in the main image.\\n4. Upload the CSV file to the Lakehouse.\\n5. Convert the file to a Delta table. \\n6. Generate a Dataset and create a Power BI report.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"query\": \"What are the steps to load a CSV file to a delta table in Microsoft Fabric?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
