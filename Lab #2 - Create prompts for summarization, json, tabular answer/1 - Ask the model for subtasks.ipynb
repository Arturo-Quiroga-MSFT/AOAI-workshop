{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "from IPython.display import display, HTML, JSON, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "OPENAI_DAVINCI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DAVINCI_DEPLOYMENT_NAME\")\n",
    "OPENAI_DAVINCI_MODEL_NAME = os.getenv(\"OPENAI_DAVINCI_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "             openai_api_version=OPENAI_DEPLOYMENT_VERSION,\n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             top_p=1,\n",
    "             ):\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      openai_api_version=openai_api_version,\n",
    "                      temperature=temperature,\n",
    "                      max_tokens=max_tokens,\n",
    "                      top_p=top_p,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "                      )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's do some math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='aidemos-workshop.openai.azure.com', port=443): Max retries exceeded with url: //openai/deployments/gpt-35-turbo/completions?api-version=2023-03-15-preview (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027453611330>: Failed to resolve 'aidemos-workshop.openai.azure.com' ([Errno 11001] getaddrinfo failed)\")).\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='aidemos-workshop.openai.azure.com', port=443): Max retries exceeded with url: //openai/deployments/gpt-35-turbo/completions?api-version=2023-03-15-preview (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027453612C20>: Failed to resolve 'aidemos-workshop.openai.azure.com' ([Errno 11002] getaddrinfo failed)\")).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Correct"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# openAI gets this wrong!!\n",
    "prompt = f\"\"\" Determine if the student's solution delimited by the triple backticks is correct or not.\n",
    "Question: When I was 2 years old my sister was twice my age. I'm now 40 years old how old is my sister now? \n",
    "Student's answer: ```The sister is now 80 years old.``` \n",
    "If student is correct, then write 'Correct', otherwise write 'Incorrect'.  \n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(prompt)\n",
    "display(HTML(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask the model to solve it and then to compare both solutions and conclude which is correct.\n",
    " Define the task as a list of sub-tasks and ask the model to perform them in a specific order (splitting the task into sub-tasks technique).   \n",
    " Then ask the model to compare its own solution with the solution provided by the model and conclude which one is correct.\n",
    " Ask the model to share its reasoning for the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Actual solution steps: \n",
       "- When I was 2 years old, my sister was 4 years old (2 * 2 = 4)\n",
       "- The age difference between us is 2 years (4 - 2 = 2)\n",
       "- Therefore, my sister is 42 years old now (40 + 2 = 42)\n",
       "\n",
       "Student's solution: The sister is now 80 years old.\n",
       "\n",
       "Student's solution is correct: False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Question = f\"\"\"When I was 2 years old my sister was twice of my age . I'm now 40 years old, how old is my sister now?\"\"\"\n",
    "Student_Solution = f\"\"\" The sister is now 80 years old.\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Determine if the Student's Solution for the Question is correct or not.\n",
    "To solve the problem do the following:\n",
    "1 - First, work out your OWN solution to the problem. Evaluate your final result to make sure it is correct and adheres to the question's conditions. \n",
    "    Reason about every step of your solution and make sure it is correct. \n",
    "2 - Second, compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Student's Solution: ```{Student_Solution}```\n",
    "Question: ```{Question}```\n",
    "\n",
    "\n",
    "Use the following output format:\n",
    "Actual solution steps: <your own solution steps>\n",
    "\\nStudent's solution: <student's solution>\n",
    "\\nStudent's solution is correct: <true/false>\n",
    "<|im_end|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
