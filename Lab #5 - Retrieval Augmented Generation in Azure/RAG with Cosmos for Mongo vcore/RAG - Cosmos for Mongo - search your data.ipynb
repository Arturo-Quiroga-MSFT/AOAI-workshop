{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Azure Cosmos DB for Mongo vcore\n",
    "You can run this notebook after running succesfully the \"RAG - Cosmos for Mongo - create embeddings\" notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import os\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "OPENAI_GPT4V_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT4V_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_DALLE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DALLE_DEPLOYMENT_NAME\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#configure Cosmos \n",
    "COSMOS_MONGO_CONNECTION_STRING = os.getenv(\"COSMOS_MONGO_CONNECTION_STRING\")\n",
    "COSMOS_INDEX_NAME = os.getenv(\"COSMOS_INDEX_NAME\")\n",
    "COSMOS_DBNAME = os.getenv(\"COSMOS_DBNAME\")\n",
    "COSMOS_COLLECTION_NAME = os.getenv(\"COSMOS_COLLECTION_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingmodel = AzureOpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def calc_embeddings(text):\n",
    "    deployment = OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return embeddingmodel.embed_query(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    ")\n",
    "\n",
    "client: MongoClient = MongoClient(COSMOS_MONGO_CONNECTION_STRING)\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "db = client[COSMOS_DBNAME]\n",
    "if COSMOS_DBNAME not in client.list_database_names():\n",
    "    # Create a database with 400 RU throughput that can be shared across\n",
    "    # the DB's collections\n",
    "    db = client[COSMOS_DBNAME]\n",
    "    print(\"Created db '{}'.\\n\".format(COSMOS_DBNAME))\n",
    "else:\n",
    "    print(\"Using database: '{}'.\\n\".format(COSMOS_DBNAME))\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "collection = db[COSMOS_COLLECTION_NAME]\n",
    "if COSMOS_COLLECTION_NAME not in db.list_collection_names():\n",
    "    # Creates a unsharded collection that uses the DBs shared throughput\n",
    "    collection = db[COSMOS_COLLECTION_NAME]\n",
    "    print(\"Created collection '{}'.\\n\".format(COSMOS_COLLECTION_NAME))\n",
    "else:\n",
    "    print(\"Using collection: '{}'.\\n\".format(COSMOS_COLLECTION_NAME))\n",
    "\n",
    "collection = client[COSMOS_DBNAME][COSMOS_COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = AzureCosmosDBVectorSearch(\n",
    "    collection, embeddingmodel, index_name=COSMOS_INDEX_NAME\n",
    ")\n",
    "\n",
    "# perform a similarity search between a query and the ingested documents\n",
    "question = \"Why does the coffin prepared for Queequeg become Ishmael's life buoy once the Pequod sinks?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "answer = docs[0].page_content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT to answer a question based on the question and the answer from our similarity search\n",
    "from openai import AzureOpenAI\n",
    "clientOpenAI = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "def call_openAI(text):\n",
    "    response = clientOpenAI.chat.completions.create(\n",
    "        model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(HTML(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
